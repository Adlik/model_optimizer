main_file: "examples/classifier_imagenet/main_quantization_distillation.py"
arch: "efficientnet_b0"
model_source: TorchVision
log_name: "qat_per_channel_distillation_load_78.064"
debug: false
data: "/data/imagenet/imagenet-torch"
weight: "/models_zoo/efficientnet_b0_multi_gpu_202210040928_acc_distill_720epoch_acc_78.064/efficientnet_b0best.pth.tar"
lr: 0.0002
epochs: 360
batch_size: 32
workers: 8
print_freq: 50
evaluate: false
pretrained: false
save_jit_trace: true
seed: 0
gpu_id: ANY
multi_gpu {
  world_size: 1
  rank: 0
  dist_url: "tcp://127.0.0.1:23457"
  dist_backend: "nccl"
  multiprocessing_distributed: true
}

lr_scheduler: CosineAnnealingLR

optimizer: RMSprop
rmsprop{
  weight_decay: 1e-5
  momentum: 0.9
}

distill {
  teacher_model {
    arch: "resnet50d"
    source: Timm
  }
  kl_divergence {
    temperature: 1.0
    reduction: "batchmean"
    loss_weight: 0.7
  }
}

quantization {
  quantize: true
  quantize_fx: true
  post_training_quantize: false
  backend: "fbgemm"
  num_observer_update_epochs: 4
  num_batch_norm_update_epochs: 99999
  activation_quantization_observer {
    quantization_method: "moving_average_minmax"
    per_channel: false
    symmetric: false
    reduce_range: true
    dtype: "quint8"
    nbits: 8
  }
  weight_quantization_observer {
    quantization_method: "moving_average_minmax"
    per_channel: true
    symmetric: true
    reduce_range: false
    dtype: "qint8"
    nbits: 8
  }
}
